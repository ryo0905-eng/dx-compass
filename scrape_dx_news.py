import requests
from bs4 import BeautifulSoup
from save_data import insert_dx_case

# PR TIMESã®DXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹URL
PR_TIMES_URL = "https://prtimes.jp/main/action.php?run=html&page=searchkey&search_word=DX%E4%BA%8B%E4%BE%8B"

def scrape_pr_times():
    print("ğŸ” PR TIMESã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—é–‹å§‹...")
    response = requests.get(PR_TIMES_URL)

    if response.status_code != 200:
        print(f"âŒ PR TIMESã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}ï¼‰")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.find_all("article", class_="release-card_article__No7uQ")

    print(f"ğŸ“ {len(articles)} ä»¶ã®DXäº‹ä¾‹ã‚’å–å¾—")

    for article in articles[:5]:
        title = article.find("h3").get_text(strip=True) if article.find("h3") else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
        url = article.find("a")["href"] if article.find("a") else "URLãªã—"
        summary = article.find("p").get_text(strip=True) if article.find("p") else "æ¦‚è¦ãªã—"

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—: {title} ({url})")
        insert_dx_case(title, url, summary, "æœªåˆ†é¡", "æœªåˆ†é¡", "ä¸æ˜")

if __name__ == "__main__":
    scrape_pr_times()
